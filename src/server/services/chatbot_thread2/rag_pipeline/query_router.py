"""
Query Router Module - Ph√¢n lo·∫°i query tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh c√≥ c·∫ßn ch·∫°y RAG pipeline kh√¥ng.

Best Practice: Ti·∫øt ki·ªám API calls v√† th·ªùi gian b·∫±ng c√°ch l·ªçc c√°c query ƒë∆°n gi·∫£n.
"""

from langchain_core.prompts import ChatPromptTemplate
from google.api_core.exceptions import ResourceExhausted
import logging

from .. import config
from .llm_factory import get_next_api_key, _create_llm_with_retry
from langchain_google_genai import ChatGoogleGenerativeAI

logger = logging.getLogger(__name__)

# --- ROUTER PROMPT (MULTILINGUAL) ---
router_prompt = ChatPromptTemplate.from_messages([
    ("system", 
     "You are an intelligent multilingual query classifier for a study abroad scholarship chatbot.\n"
     "The user's query can be in ANY language (English, Vietnamese, Japanese, Korean, Chinese, etc.).\n"
     "Your task is to classify it into ONE of these categories:\n\n"
     
     "1. **greeting**: Simple greetings or introductions in any language\n"
     "2. **scholarship_search**: Questions about scholarships, studying abroad, funding, applications\n"
     "3. **chitchat**: Casual conversation, thanks, small talk (NOT scholarship related)\n"
     "4. **off_topic**: Questions unrelated to scholarships or education\n\n"
     
     "RULES:\n"
     "- ANY scholarship/study abroad keyword ‚Üí 'scholarship_search'\n"
     "- Follow-ups like 'tell me more' ‚Üí 'scholarship_search'\n"
     "- Only pure greetings/thanks get 'greeting'/'chitchat'"
    ),
    ("human", "Classify this query: {user_query}")
])

def get_router_llm() -> ChatGoogleGenerativeAI:
    """
    T·∫°o LLM cho Router (s·ª≠ d·ª•ng API key rotation v·ªõi auto-retry).
    """
    def creator(api_key: str) -> ChatGoogleGenerativeAI:
        llm = ChatGoogleGenerativeAI(
            model=config.ROUTER_LLM_MODEL,
            google_api_key=api_key,
            temperature=config.ROUTER_LLM_TEMP
        )
        return llm.with_structured_output(config.QueryClassification)
    return _create_llm_with_retry(creator)

def classify_query(user_query: str) -> config.QueryClassification:
    """
    Ph√¢n lo·∫°i query c·ªßa user v·ªõi retry logic.
    H·ªó tr·ª£ ƒëa ng√¥n ng·ªØ (kh√¥ng c·∫ßn d·ªãch tr∆∞·ªõc).
    T·ª± ƒë·ªông skip key h·∫øt quota v√† th·ª≠ key ti·∫øp theo.
    
    Args:
        user_query: C√¢u h·ªèi g·ªëc c·ªßa user (b·∫•t k·ª≥ ng√¥n ng·ªØ n√†o)
        
    Returns:
        QueryClassification object v·ªõi query_type v√† reasoning
    """
    logger.info(f"--- [ROUTER] Classifying query: '{user_query[:100]}...' ---")
    
    from .llm_factory import API_KEY_POOL
    max_attempts = len(API_KEY_POOL)
    last_error = None
    
    for attempt in range(max_attempts):
        try:
            # T·∫°o LLM v√† chain M·ªöI m·ªói l·∫ßn th·ª≠
            router_llm = get_router_llm()
            router_chain = router_prompt | router_llm
            
            # Invoke chain tr·ª±c ti·∫øp (kh√¥ng c·∫ßn translate)
            classification = router_chain.invoke({"user_query": user_query})
            
            logger.info(f"--- [ROUTER] Classification: {classification.query_type} ---")
            logger.info(f"--- [ROUTER] Reasoning: {classification.reasoning} ---")
            
            return classification
            
        except ResourceExhausted as e:
            last_error = e
            logger.warning(
                f"‚ö†Ô∏è [ROUTER] API Key h·∫øt quota (429). "
                f"Th·ª≠ key ti·∫øp theo... (Attempt {attempt + 1}/{max_attempts})"
            )
            
            if attempt < max_attempts - 1:
                continue  # Th·ª≠ l·∫°i v·ªõi key m·ªõi
            else:
                # ƒê√£ th·ª≠ h·∫øt t·∫•t c·∫£ keys
                logger.error(f"‚ùå [ROUTER] T·∫§T C·∫¢ {max_attempts} API keys ƒë·ªÅu h·∫øt quota!")
                raise ResourceExhausted(
                    f"All API keys exceeded quota. Please check billing."
                ) from last_error
                
        except Exception as e:
            # L·ªói kh√°c kh√¥ng retry
            logger.error(f"‚ùå [ROUTER] L·ªói kh√¥ng mong mu·ªën: {e}")
            raise
    
    # Fallback (kh√¥ng n√™n reach ƒë∆∞·ª£c ƒë√¢y)
    raise RuntimeError("classify_query: Unexpected state")

def should_use_rag(classification: config.QueryClassification) -> bool:
    """
    Quy·∫øt ƒë·ªãnh c√≥ c·∫ßn ch·∫°y RAG pipeline kh√¥ng.
    
    Returns:
        True: C·∫ßn query vector store v√† ch·∫°y full RAG
        False: Kh√¥ng c·∫ßn RAG, tr·∫£ l·ªùi tr·ª±c ti·∫øp
    """
    # Ch·ªâ scholarship_search m·ªõi c·∫ßn RAG
    needs_rag = classification.query_type == "scholarship_search"
    
    logger.info(f"--- [ROUTER] Needs RAG: {needs_rag} ---")
    return needs_rag

# --- DYNAMIC RESPONSE GENERATION (Multilingual) ---
response_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a friendly study abroad scholarship advisor chatbot.\n"
     "Generate a SHORT, appropriate response based on the query type.\n\n"
     "Response templates:\n"
     "- greeting: Greet warmly, introduce yourself as scholarship advisor, ask what they need\n"
     "- chitchat: Thank them politely, remind you can help with scholarships\n"
     "- off_topic: Apologize politely, state you only help with scholarships, ask if they have scholarship questions\n\n"
     "CRITICAL: Respond in the SAME LANGUAGE as the user's original query.\n"
     "Keep it concise (2-3 sentences max). Use emoji if appropriate (üëãüòäüéì)."
    ),
    ("human", "Query type: {query_type}\nUser query: {user_query}\n\nGenerate response:")
])

def get_direct_response(classification: config.QueryClassification, user_query: str) -> str:
    """
    Sinh c√¢u tr·∫£ l·ªùi ƒë·ªông b·∫±ng LLM (h·ªó tr·ª£ m·ªçi ng√¥n ng·ªØ).
    
    Args:
        classification: K·∫øt qu·∫£ ph√¢n lo·∫°i t·ª´ router
        user_query: Query g·ªëc c·ªßa user
        
    Returns:
        C√¢u tr·∫£ l·ªùi b·∫±ng ng√¥n ng·ªØ c·ªßa user
    """
    from langchain_core.output_parsers import StrOutputParser
    from .llm_factory import get_translator_llm  # D√πng chung translator LLM (nh·∫π)
    
    try:
        llm = get_translator_llm()  # Flash model, nhanh
        chain = response_prompt | llm | StrOutputParser()
        
        response = chain.invoke({
            "query_type": classification.query_type,
            "user_query": user_query
        })
        
        return response.strip()
        
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        # Fallback ti·∫øng Anh n·∫øu LLM fail
        return "Hello! I'm your scholarship advisor. How can I help you find scholarships?"

if __name__ == '__main__':
    # Test router v·ªõi 50 queries ƒë·ªÉ ki·ªÉm tra quota limits
    test_queries = [
        # Greetings (10)
        "xin ch√†o b·∫°n",
        "hello there",
        "hi",
        "ch√†o bu·ªïi s√°ng",
        "good morning",
        "hey",
        "xin ch√†o",
        "hello",
        "ch√†o b·∫°n",
        "„Åì„Çì„Å´„Å°„ÅØ"
    ]
    
    print(f"\nüß™ TESTING ROUTER WITH {len(test_queries)} QUERIES")
    print(f"{'='*80}\n")
    
    # Tracking stats
    stats = {
        "greeting": 0,
        "scholarship_search": 0,
        "chitchat": 0,
        "off_topic": 0
    }
    quota_errors = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[{i}/{len(test_queries)}] Query: {query}")
        
        try:
            classification = classify_query(query)
            stats[classification.query_type] += 1
            
            print(f"‚úÖ Type: {classification.query_type}")
            print(f"   Reasoning: {classification.reasoning[:80]}...")
            
            if not should_use_rag(classification):
                response = get_direct_response(classification, query)
                print(f"   Direct Response: {response[:60]}...")
                
        except ResourceExhausted as e:
            print(f"‚ùå ALL KEYS EXHAUSTED at query #{i}")
            quota_errors.append(i)
            break
        except Exception as e:
            print(f"‚ùå ERROR: {str(e)[:100]}")
            break
    
    # Print summary
    print(f"\n{'='*80}")
    print(f"TEST SUMMARY")
    print(f"{'='*80}")
    print(f"Total queries tested: {i}/{len(test_queries)}")
    print(f"\nClassification breakdown:")
    for query_type, count in stats.items():
        print(f"  - {query_type}: {count}")
    
    if quota_errors:
        print(f"\n‚ö†Ô∏è Quota errors at queries: {quota_errors}")
    else:
        print(f"\n‚úÖ All queries completed successfully!")
